{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install -U langchain langchain-core langchain-community langchain-openai azure-search-documents azure-identity",
   "id": "15305232748419a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 환경 설정 Import\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "ff14a557be8b3984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# KT 고객센터 상담 시나리오 FAQ - LCEL Chain 으로 만들기",
   "id": "d0af433087333a79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 데이터 출처: https://ermsweb.kt.com/pc/faq/faqList.do\n",
    "import json\n",
    "category = [\n",
    "    \"USIM\",\n",
    "    \"모바일\",\n",
    "    \"혜택\",\n",
    "    \"결합\",\n",
    "    \"인터넷\",\n",
    "    \"tv\",\n",
    "    \"집전화\",\n",
    "    \"인터넷전화\",\n",
    "    \"IoT\",\n",
    "    \"Egg\",\n",
    "    \"와이파이\",\n",
    "    \"기타\",\n",
    "    \"SHOP\",\n",
    "]\n",
    "\n",
    "def load_faq_data(file_path=\"faq_data.json\"):\n",
    "    \"\"\"FAQ 데이터를 로드하는 함수\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            result = json.load(f)\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        print(f\"[ERROR] JSONDecode: {e}\")\n",
    "    return result\n",
    "\n",
    "# 데이터 로드\n",
    "faq_data = load_faq_data()"
   ],
   "id": "6488e9c76e37b542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. FAQ 데이터 전처리",
   "id": "ae3df1b509dc5bce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Load and Parse 를 할 필요가 없음 -> 이미 다 파싱되어 JSON 으로 있습니다\"\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(category)\n",
    "print(\"==\" * 100)\n",
    "pprint(faq_data)"
   ],
   "id": "8868f8ffc65480c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 LangChain Document 객체 이해",
   "id": "deefebab933cbc29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "sample_doc = Document(\n",
    "    page_content=\"이것은 문서의 내용입니다.\\n실제 텍스트가 여기에 들어가게 되고, 이 내용 전체가 Embedding 대상인 Chunk 입니다.\",\n",
    "    metadata={\n",
    "        \"source\": \"faq\",\n",
    "        \"category\": \"예제\",\n",
    "        \"id\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"<<LangChain Document 구조>>\")\n",
    "print(f\"- page_content: {sample_doc.page_content}\")\n",
    "print(f\"- metadata: {sample_doc.metadata} \\n  메타데이터는 Filtering 에 사용됩니다. (RDB 의 Where 조건절)\")"
   ],
   "id": "68a6c3dcc6dbe34d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. FAQ 데이터 Embedding",
   "id": "7a8c03558f807502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    "    openai_api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "#Azure AI Search(VectorDB)\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=f\"https://{os.environ['AZURE_AI_SEARCH_SERVICE_NAME']}.search.windows.net\",\n",
    "    azure_search_key=os.environ[\"AZURE_AI_SEARCH_API_KEY\"],\n",
    "    index_name=\"faq-index\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(embeddings)\n",
    "print(vector_store)"
   ],
   "id": "e0b313ca79138157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# JSON to Langchain Document\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def json_to_document(json_data: dict):\n",
    "    documents = []\n",
    "    for cat, qa_list in json_data.items():\n",
    "        print(f\"Category: {cat}\")\n",
    "        for _, qa_item in enumerate(qa_list):\n",
    "                doc = Document(\n",
    "                    page_content=f\"Category: {cat}\\nQuestion: {qa_item['question']}\\nAnswer: {qa_item['answer']}\",\n",
    "                    metadata={\n",
    "                        \"category\": cat,\n",
    "                        \"source\": \"faq\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "docs = json_to_document(faq_data)\n",
    "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "\n",
    "# Vector DB 에 저장\n",
    "vector_store.add_documents(documents=docs, ids=uuids)"
   ],
   "id": "2210d7591ea23812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 단순 유사도 검색\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "def search_simple_cos(input_vector_store: VectorStore, query: str, k: int = 3):\n",
    "    _results = input_vector_store.similarity_search(query, k=k)\n",
    "    return _results\n",
    "\n",
    "test_question = \"usim\"\n",
    "results = search_simple_cos(vector_store, test_question, k=1)\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ],
   "id": "441304e8b40177ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 단순 Cosine Similarity with Score (스코어까지 보여줌)\n",
    "\n",
    "def search_simple_cos_with_score(input_vector_store: VectorStore, query: str, k: int = 3):\n",
    "    _results = input_vector_store.similarity_search_with_score(query, k=k)\n",
    "    return _results\n",
    "\n",
    "score_question = \"인터넷\"\n",
    "results = search_simple_cos_with_score(vector_store, score_question, k=1)\n",
    "for res, score in results:\n",
    "    print(f\"* [SCORE={score:3f}]\\n{res.page_content} [{res.metadata}]\")"
   ],
   "id": "f77bb39c878ade86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retriever Customizing 실무용\n",
    "def custom_retriever(k: int = 3, fetch_k: int = 5, filter: dict | None = None):\n",
    "    result = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": k, \"fetch_k\": fetch_k,\n",
    "                       \"filter\": filter}\n",
    "    )\n",
    "    return result\n",
    "\n",
    "_custom_r = custom_retriever()\n",
    "\n",
    "real_question = \"USIM가입 절차는?\"\n",
    "real_filter = {\n",
    "    \"category\": \"USIM가입\"\n",
    "}\n",
    "_custom_r.invoke(real_question, filter=real_filter)"
   ],
   "id": "f68252eccf4727a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Hybrid Search (Semantic + **Lexical**)",
   "id": "2874a2181982463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lexical Search Retriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# 설치 필요\n",
    "# %pip install rank_bm25\n",
    "\n",
    "bm25_rr = BM25Retriever.from_documents(docs)\n",
    "bm25_rr.invoke(\"인터넷 요금이 왜 이렇게 많이 나왔나요?\", k=3)"
   ],
   "id": "fe5b3bd10690b09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 형태소 분석기 설치\n",
    "# %pip install kiwipiepy konlpy"
   ],
   "id": "cc48b6b5ab49fff5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 그래서 어떤 Lexical(TF-IDF 기반) Search Retriever 를 쓰면 되는지?\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_rr = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        bm25_rr,  # Lexical Retriever\n",
    "        vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})  # Vector Retriever\n",
    "    ],\n",
    "    weights=[0.5, 0.5]  # 각 검색기의 가중치\n",
    ")"
   ],
   "id": "2db78779e6da107c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ensemble_rr.invoke(\"인터넷 요금이 왜 이렇게 많이 나왔나요?\")",
   "id": "78241976485f6897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 추천하는 방식은 한글 형태소 분석기가 달린 BM25(TF-IDF 기반) 검색기를 사용을 기본으로 하는 Custom Retriever 를 만드는 것\n",
    "# NOTE: 바로 가져다 쓰셔도 됩니다^^\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import Field\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "try:\n",
    "    from kiwipiepy import Kiwi\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Could not import kiwipiepy, please install with `pip install kiwipiepy`.\"\n",
    "    )\n",
    "\n",
    "kiwi_tokenizer = Kiwi()\n",
    "\n",
    "\n",
    "def kiwi_preprocessing_func(text: str) -> List[str]:\n",
    "    return [token.form for token in kiwi_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "def default_preprocessing_func(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "class KiwiBM25Retriever(BaseRetriever):\n",
    "    \"\"\"`BM25` retriever without Elasticsearch and Add Kiwi Tokenizer\"\"\"\n",
    "\n",
    "    vectorizer: Any\n",
    "    \"\"\" BM25 vectorizer.\"\"\"\n",
    "    docs: List[Document] = Field(repr=False)\n",
    "    \"\"\" List of documents.\"\"\"\n",
    "    k: int = 5\n",
    "    \"\"\" Number of documents to return.\"\"\"\n",
    "    preprocess_func: Callable[[str], List[str]] = kiwi_preprocessing_func\n",
    "    \"\"\" Preprocessing function to use on the text before BM25 vectorization.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_texts(\n",
    "        cls,\n",
    "        texts: Iterable[str],\n",
    "        meta_data: Optional[Iterable[dict]] = None,\n",
    "        bm25_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Callable[[str], List[str]] = kiwi_preprocessing_func,\n",
    "        **kwargs: Any,\n",
    "    ) -> KiwiBM25Retriever:\n",
    "        \"\"\"\n",
    "        Create a KiwiBM25Retriever from a list of texts.\n",
    "        Args:\n",
    "            texts: A list of texts to vectorize.\n",
    "            meta_data: A list of metadata dicts to associate with each text.\n",
    "            bm25_params: Parameters to pass to the BM25 vectorizer.\n",
    "            preprocess_func: A function to preprocess each text before vectorization.\n",
    "            **kwargs: Any other arguments to pass to the retriever.\n",
    "\n",
    "        Returns:\n",
    "            A KiwiBM25Retriever instance.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from rank_bm25 import BM25Okapi\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Could not import rank_bm25, please install with `pip install \"\n",
    "                \"rank_bm25`.\"\n",
    "            )\n",
    "\n",
    "        texts_processed = [preprocess_func(t) for t in texts]\n",
    "        bm25_params = bm25_params or {}\n",
    "        vectorizer = BM25Okapi(texts_processed, **bm25_params)\n",
    "        meta_data = meta_data or ({} for _ in texts)\n",
    "        docs = [Document(page_content=t, metadata=m) for t, m in zip(texts, meta_data)]\n",
    "        return cls(\n",
    "            vectorizer=vectorizer, docs=docs, preprocess_func=preprocess_func, **kwargs\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        documents: Iterable[Document],\n",
    "        *,\n",
    "        bm25_params: Optional[Dict[str, Any]] = None,\n",
    "        preprocess_func: Callable[[str], List[str]] = kiwi_preprocessing_func,\n",
    "        **kwargs: Any,\n",
    "    ) -> KiwiBM25Retriever:\n",
    "        \"\"\"\n",
    "        Create a KiwiBM25Retriever from a list of Documents.\n",
    "        Args:\n",
    "            documents: A list of Documents to vectorize.\n",
    "            bm25_params: Parameters to pass to the BM25 vectorizer.\n",
    "            preprocess_func: A function to preprocess each text before vectorization.\n",
    "            **kwargs: Any other arguments to pass to the retriever.\n",
    "\n",
    "        Returns:\n",
    "            A KiwiBM25Retriever instance.\n",
    "        \"\"\"\n",
    "        texts, meta_data = zip(*((d.page_content, d.metadata) for d in documents))\n",
    "        return cls.from_texts(\n",
    "            texts=texts,\n",
    "            bm25_params=bm25_params,\n",
    "            metadatas=meta_data,\n",
    "            preprocess_func=preprocess_func,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        processed_query = self.preprocess_func(query)\n",
    "        return_docs = self.vectorizer.get_top_n(processed_query, self.docs, n=self.k)\n",
    "        return return_docs\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def argsort(seq, reverse):\n",
    "        return sorted(range(len(seq)), key=seq.__getitem__, reverse=reverse)\n",
    "\n",
    "    def search_with_score(self, query: str, top_k=None):\n",
    "        normalized_score = KiwiBM25Retriever.softmax(\n",
    "            self.vectorizer.get_scores(self.preprocess_func(query))\n",
    "        )\n",
    "\n",
    "        if top_k is None:\n",
    "            top_k = self.k\n",
    "\n",
    "        score_indexes = KiwiBM25Retriever.argsort(normalized_score, True)\n",
    "\n",
    "        docs_with_scores = []\n",
    "        for i, doc in enumerate(self.docs):\n",
    "            document = Document(\n",
    "                page_content=doc.page_content, metadata={\"score\": normalized_score[i]}\n",
    "            )\n",
    "            docs_with_scores.append(document)\n",
    "\n",
    "        score_indexes = score_indexes[:top_k]\n",
    "\n",
    "        # Creating an itemgetter object\n",
    "        getter = itemgetter(*score_indexes)\n",
    "\n",
    "        # Using itemgetter to get items\n",
    "        selected_elements = getter(docs_with_scores)\n",
    "        return selected_elements"
   ],
   "id": "f36409d3ccf99173",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 그럼 최종적으로 Hybrid Search Retriever 를 EnsembleRetriever 로 만들어본다면?\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        KiwiBM25Retriever.from_documents(docs),  # Kiwi BM25 Retriever\n",
    "        vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})  # Vector Retriever\n",
    "    ],\n",
    "    weights=[0.7, 0.3]  # 각 검색기의 가중치\n",
    ")\n",
    "\n",
    "\n",
    "hybrid_retriever.invoke(\"인터넷 요금이 왜 이렇게 많이 나왔나요?\")"
   ],
   "id": "93c5717425f75c59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 텍스트 분할(Text Splitting)",
   "id": "a4358ebf073ecf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # 청크 크기(500 이하로 추천)\n",
    "    chunk_overlap=50,    # 청크 간 겹침\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # 분할 우선 순위\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"✅ {len(docs)}개 문서 → {len(split_docs)}개 청크로 분할\")\n",
    "\n",
    "# 분할 결과 확인\n",
    "print(\"\\n🔍 분할 예시:\")\n",
    "print(f\"원본 문서 길이: {len(docs[0].page_content)}자\")\n",
    "if len(split_docs) > len(docs):\n",
    "    print(f\"분할된 첫 번째 청크: {split_docs[0].page_content}\")"
   ],
   "id": "4bdaa7a5511195cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. FAQ LCEL Chain 만들기",
   "id": "52564d00f0a500d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_docs(docs: list[Document]) -> str:\n",
    "    \"\"\"\n",
    "    검색된 문서들을 하나의 문자열로 포맷팅\n",
    "\n",
    "    Args:\n",
    "        docs: Document 객체 리스트\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        formatted.append(f\"[Document Context_{i}]\\n{doc.page_content}]\")\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)"
   ],
   "id": "e03e414f007b0780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sys_prompt = \"\"\"당신은 KT 고객센터 상담원입니다.\n",
    "무조건 아래 참고 자료를 기반으로 하여 고객의 질문에 친절하고 정확하게 제공된 자료만을 사용하여 답변하세요.\n",
    "\n",
    "-----\n",
    "참고 자료:\n",
    "{context}\n",
    "-----\n",
    "\"\"\""
   ],
   "id": "9bb5b7b39dab495c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(sys_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "prompt"
   ],
   "id": "f036cb30bb8f3b52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "###\n",
    "deployment = \"gpt-4.1-nano\" # 모델 Deployment 이름\n",
    "api_version=\"2025-01-01-preview\"\n",
    "###\n",
    "\n",
    "rag_model = AzureChatOpenAI(\n",
    "    azure_deployment=deployment,\n",
    "    api_version=api_version,\n",
    "    temperature=0.1,\n",
    ")"
   ],
   "id": "86c9894bf4d9d877",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chain 생성\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "naive_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | rag_model\n",
    "\n",
    "naive_chain"
   ],
   "id": "bb65491ee3b18228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. FAQ Chain 실행",
   "id": "15570b2cdcf0db66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_question = \"인터넷이 자꾸 끊겨요. 어떻게 해야 하나요?\"\n",
    "response = naive_chain.invoke(user_question)\n",
    "\n",
    "response.pretty_print()"
   ],
   "id": "4607d2320752c008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. FAQ Chain 업그레이드",
   "id": "b15b27ee93e39200"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Few-shot Prompt",
   "id": "9ae758b848578545"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"휴대폰 요금이 너무 비싸요\",\n",
    "        \"output\": \"\"\"고객님, 휴대폰 요금 부담을 덜어드릴 수 있는 방법을 안내해드리겠습니다.\n",
    "\n",
    "1. **요금제 변경**: 현재 사용량에 맞는 요금제로 변경하시면 절약 가능합니다.\n",
    "2. **가족 결합**: 가족과 함께 결합하시면 추가 할인을 받으실 수 있습니다.\n",
    "3. **멤버십 할인**: KT 멤버십 등급에 따라 할인 혜택이 있습니다.\n",
    "\n",
    "고객님의 현재 요금제와 사용 패턴을 확인하여 최적의 방안을 찾아드리겠습니다.\n",
    "추가로 궁금하신 점이 있으시면 말씀해 주세요.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"인터넷이 안 돼요\",\n",
    "        \"output\": \"\"\"고객님, 인터넷 연결 문제로 불편을 드려 죄송합니다.\n",
    "다음 단계를 순서대로 확인해 주세요:\n",
    "\n",
    "1. **모뎀/공유기 확인**\n",
    "   - 전원이 켜져 있는지 확인\n",
    "   - 모든 케이블이 제대로 연결되어 있는지 확인\n",
    "\n",
    "2. **재시작**\n",
    "   - 모뎀과 공유기의 전원을 뺐다가 30초 후 다시 연결\n",
    "\n",
    "3. **기기 설정**\n",
    "   - WiFi가 켜져 있는지 확인\n",
    "   - 올바른 네트워크에 연결되어 있는지 확인\n",
    "\n",
    "위 방법으로도 해결되지 않으면 고객센터(100번)로 연락 주시면\n",
    "원격 점검을 도와드리겠습니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Few-shot 프롬프트 생성\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# 최종 프롬프트\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 10년 경력의 친절한 KT 고객센터 상담원입니다.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"\"\"참고 자료:\n",
    "{context}\n",
    "\n",
    "고객 질문: {question}\"\"\")\n",
    "])\n",
    "\n",
    "# Few-shot RAG 체인\n",
    "few_shot_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | rag_model\n",
    ")\n",
    "few_shot_rag_chain"
   ],
   "id": "2e9ce4d4e9f12ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "few_shot_test_question = \"내 인터넷 요금이 왜 이렇게 많이 나왔나요?\"\n",
    "few_shot_response = few_shot_rag_chain.invoke(few_shot_test_question)\n",
    "\n",
    "few_shot_response.pretty_print()"
   ],
   "id": "883dfe51d6756fd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 메모리",
   "id": "bd84a19285652a77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 주의: 현재 LangChain 에서 제공하는 Conversation Memory 는 모두 Deprecated 되었습니다.\n",
    "# Conversation 메모리의 개념을 이해하고 활용하는 것으로만 활용하시고, 실제 업무에는 사용하시면 안됩니다.\n",
    "\n"
   ],
   "id": "c1332fa82a9038db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Chain 전략 다변화",
   "id": "7cca3f343f77d1f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.1 질문 의도 분석 체인 추가",
   "id": "782cf94667b31d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 의도 분석 프롬프트\n",
    "intent_analysis_sys_prompt = \"\"\"\n",
    "당신은 KT 고객센터의 상담 전문가이면서 의도 분석에 관한 최고 전문가입니다.\n",
    "고객의 질문을 깊이 있게 한국어의 맥락을 차분히 파악, 분석하여 정확한 의도를 파악해주세요.\n",
    "\n",
    "## 분석 기준:\n",
    "1. 주요 의도: 고객이 원하는 핵심 목적\n",
    "2. 긴급도: 문제의 시급성 (high/medium/low)\n",
    "3. 감정: 고객의 감정 상태 (positive/neutral/negative)\n",
    "4. 상담원 필요: 자동 응답으로 해결이 불가능한 경우\n",
    "\n",
    "\n",
    "## 의도 카테고리는 다음과 같습니다:\n",
    "{intent_categories}\n",
    "\n",
    "## Output Format:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "intent_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", intent_analysis_sys_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")"
   ],
   "id": "d3106e55d5afd521",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class UserIntent(str, Enum):\n",
    "    \"\"\"사용자 의도 분류\"\"\"\n",
    "    INFO_INQUIRY = \"info_inquiry\"              # 정보 문의\n",
    "    SERVICE_APPLICATION = \"service_application\" # 서비스 신청\n",
    "    SERVICE_CHANGE = \"service_change\"          # 서비스 변경\n",
    "    SERVICE_CANCELLATION = \"service_cancellation\" # 서비스 해지\n",
    "    BILLING_INQUIRY = \"billing_inquiry\"        # 요금 문의\n",
    "    TECHNICAL_SUPPORT = \"technical_support\"    # 기술 지원\n",
    "    COMPLAINT = \"complaint\"                    # 불만 사항\n",
    "    GENERAL_INQUIRY = \"general_inquiry\"        # 일반 문의\n",
    "\n",
    "class IntentAnalysis(BaseModel):\n",
    "    \"\"\"의도 분석 결과\"\"\"\n",
    "    intent: UserIntent = Field(description=\"주요 의도\")\n",
    "    sub_intent: str | None = Field(None, description=\"세부 의도\")\n",
    "    confidence: float = Field(description=\"신뢰도 (0.0-1.0)\", ge=0.0, le=1.0)\n",
    "    keywords: list[str] = Field(description=\"핵심 키워드(5개 이하)\")\n",
    "    urgency: Literal[\"high\", \"medium\", \"low\"] = Field(description=\"긴급도\")\n",
    "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"] = Field(description=\"감정 상태\")\n",
    "    requires_human: bool = Field(description=\"상담원 연결 필요 여부\")\n",
    "\n",
    "\n",
    "intent_parser = JsonOutputParser(pydantic_object=IntentAnalysis)"
   ],
   "id": "e9442cf88d12b52a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 의도 분석 체인\n",
    "intent_analysis_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"format_instructions\": lambda x: intent_parser.get_format_instructions(),\n",
    "        \"intent_categories\": lambda x: \", \".join([i.value for i in UserIntent])\n",
    "    }\n",
    "    | intent_chat_prompt\n",
    "    | rag_model\n",
    "    | intent_parser\n",
    ")\n",
    "intent_analysis_chain"
   ],
   "id": "ed530a3cd8d39ad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.2 감정 분석 체인 추가",
   "id": "48f98e41b0bd7e45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentiment_analysis_sys_prompt = \"\"\"당신은 고객의 감정을 분석하는 전문가입니다.\"\"\"\n",
    "sentiment_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", sentiment_analysis_sys_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")"
   ],
   "id": "f44a8bb94c0ccdb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.3 질문 의도 + 감정 분석 병렬 처리",
   "id": "eff4f2c3e4fbbf3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: 실습!",
   "id": "728d60728807b60c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.4 각 의도별로 전문화된 체인 만들기",
   "id": "1f6b7a7f373a0c86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 각 의도별 전문 체인 (예시)\n",
    "\n",
    "billing_chain = ChatPromptTemplate.from_template(\n",
    "    \"요금 전문 상담원입니다. 요금 관련 문의: {question}\"\n",
    ") | rag_model\n",
    "\n",
    "technical_chain = ChatPromptTemplate.from_template(\n",
    "    \"기술 지원팀입니다. 기술적 문제: {question}\\n단계별 해결 방법을 안내하겠습니다.\"\n",
    ") | rag_model\n",
    "\n",
    "service_chain = ChatPromptTemplate.from_template(\n",
    "    \"서비스 가입/변경 담당입니다. 문의 사항: {question}\"\n",
    ") | rag_model\n",
    "\n",
    "cancellation_chain = ChatPromptTemplate.from_template(\n",
    "    \"해지 방지 팀입니다. 고객님의 불편사항: {question}\\n혜택을 안내드리겠습니다.\"\n",
    ") | rag_model\n",
    "\n",
    "general_chain = ChatPromptTemplate.from_template(\n",
    "    \"일반 상담원입니다. 문의사항: {question}\"\n",
    ") | rag_model"
   ],
   "id": "c4cd2f7015aad54a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 의도 기반 라우팅 체인\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "def classify_intent(question: str) -> str:\n",
    "    # TODO: 과연 어떻게 만들어야 의도를 키워드로 추출할 수 있을까요?\n",
    "    return \"general\"\n",
    "\n",
    "intent_router = RunnableBranch(\n",
    "    (lambda x: classify_intent(x[\"question\"]) == \"billing\", billing_chain),\n",
    "    (lambda x: classify_intent(x[\"question\"]) == \"technical\", technical_chain),\n",
    "    (lambda x: classify_intent(x[\"question\"]) == \"service\", service_chain),\n",
    "    (lambda x: classify_intent(x[\"question\"]) == \"cancellation\", cancellation_chain),\n",
    "    general_chain  # 기본값\n",
    ")"
   ],
   "id": "a4f0e0625dc45ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 6.5 고객 정보 추가"
   ],
   "id": "3d8109efb7b09e92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CustomerContext(BaseModel):\n",
    "    \"\"\"고객 컨텍스트\"\"\"\n",
    "    customer_type: str = Field(description=\"고객 유형: VIP/일반/신규\")\n",
    "    subscription_months: int = Field(description=\"가입 기간(월)\")\n",
    "    monthly_fee: int = Field(description=\"월 요금\")\n",
    "    has_issues: bool = Field(description=\"최근 문제 발생 여부\")"
   ],
   "id": "72c875f6a6d8a307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_customer_aware_chain():\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    \"\"\"고객 인식 체인 생성\"\"\"\n",
    "\n",
    "    # VIP 전용 체인\n",
    "    vip_chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"VIP 고객님께 특별한 서비스를 제공합니다.\n",
    "        가입 기간: {subscription_months}개월\n",
    "        월 요금: {monthly_fee:,}원\n",
    "\n",
    "        문의사항: {question}\n",
    "\n",
    "        VIP 전용 혜택과 함께 최우선으로 처리해드리겠습니다.\"\"\"\n",
    "    ) | rag_model | StrOutputParser()\n",
    "\n",
    "    # 이탈 위험 고객 체인\n",
    "    churn_risk_chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"소중한 고객님, 불편을 드려 죄송합니다.\n",
    "        고객님께서 겪으신 문제를 해결하고 특별 혜택을 제공하겠습니다.\n",
    "\n",
    "        문의사항: {question}\n",
    "\n",
    "        고객님을 위한 맞춤 혜택을 준비했습니다.\"\"\"\n",
    "    ) | rag_model | StrOutputParser()\n",
    "\n",
    "    # 일반 고객 체인\n",
    "    normal_chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"안녕하세요, KT입니다.\n",
    "\n",
    "        문의사항: {question}\n",
    "\n",
    "        도움을 드리겠습니다.\"\"\"\n",
    "    ) | rag_model | StrOutputParser()\n",
    "\n",
    "    # 조건부 라우팅\n",
    "    return RunnableBranch(\n",
    "        # VIP 고객\n",
    "        (lambda x: x.get(\"customer_type\") == \"VIP\", vip_chain),\n",
    "        # 이탈 위험 고객 (오래된 고객 + 최근 문제)\n",
    "        (lambda x: x.get(\"subscription_months\", 0) > 24 and x.get(\"has_issues\", False), churn_risk_chain),\n",
    "        # 기본\n",
    "        normal_chain\n",
    "    )\n",
    "\n",
    "# 고객 인식 체인 생성\n",
    "customer_aware_chain = create_customer_aware_chain()\n",
    "\n",
    "# 다양한 고객 시나리오 테스트\n",
    "test_customers = [\n",
    "    {\n",
    "        \"customer_type\": \"VIP\",\n",
    "        \"subscription_months\": 60,\n",
    "        \"monthly_fee\": 150000,\n",
    "        \"has_issues\": False,\n",
    "        \"question\": \"해외 로밍 요금이 궁금합니다\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_type\": \"일반\",\n",
    "        \"subscription_months\": 36,\n",
    "        \"monthly_fee\": 50000,\n",
    "        \"has_issues\": True,\n",
    "        \"question\": \"서비스가 불만족스러워서 해지를 고려중입니다\"\n",
    "    },\n",
    "    {\n",
    "        \"customer_type\": \"신규\",\n",
    "        \"subscription_months\": 2,\n",
    "        \"monthly_fee\": 35000,\n",
    "        \"has_issues\": False,\n",
    "        \"question\": \"5G 요금제로 변경하고 싶어요\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"👥 고객별 맞춤 응답:\\n\")\n",
    "for customer in test_customers:\n",
    "    print(f\"고객 유형: {customer['customer_type']} (가입 {customer['subscription_months']}개월)\")\n",
    "    print(f\"Q: {customer['question']}\")\n",
    "    response = customer_aware_chain.invoke(customer)\n",
    "    print(f\"A: {response[:150]}...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "# 의도별 응답 템플릿\n",
    "class IntentRouter:\n",
    "    \"\"\"의도별 응답 라우터\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.templates = self._create_templates()\n",
    "\n",
    "    def _create_templates(self) -> Dict[UserIntent, ChatPromptTemplate]:\n",
    "        \"\"\"의도별 전문 템플릿 생성\"\"\"\n",
    "        return {\n",
    "            UserIntent.BILLING_INQUIRY: ChatPromptTemplate.from_template(\n",
    "                \"\"\"요금 전문 상담원입니다.\n",
    "                질문: {question}\n",
    "                분석: {analysis}\n",
    "\n",
    "                정확한 요금 정보와 절약 방법을 안내해드리겠습니다.\"\"\"\n",
    "            ),\n",
    "\n",
    "            UserIntent.TECHNICAL_SUPPORT: ChatPromptTemplate.from_template(\n",
    "                \"\"\"기술 지원팀입니다.\n",
    "                질문: {question}\n",
    "                긴급도: {urgency}\n",
    "\n",
    "                단계별 해결 방법을 안내하겠습니다.\"\"\"\n",
    "            ),\n",
    "\n",
    "            UserIntent.COMPLAINT: ChatPromptTemplate.from_template(\n",
    "                \"\"\"고객님의 불편을 해결하겠습니다.\n",
    "                질문: {question}\n",
    "                감정: {sentiment}\n",
    "\n",
    "                진심으로 사과드리며 즉시 개선하겠습니다.\"\"\"\n",
    "            ),\n",
    "\n",
    "            UserIntent.SERVICE_CHANGE: ChatPromptTemplate.from_template(\n",
    "                \"\"\"서비스 변경을 도와드리겠습니다.\n",
    "                질문: {question}\n",
    "\n",
    "                최적의 서비스로 안내해드리겠습니다.\"\"\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def route_and_respond(self, question: str, analysis: IntentAnalysis) -> str:\n",
    "        \"\"\"의도에 따라 적절한 응답 생성\"\"\"\n",
    "        # 기본 템플릿\n",
    "        default_template = ChatPromptTemplate.from_template(\n",
    "            \"KT입니다. {question}에 대해 도움을 드리겠습니다.\"\n",
    "        )\n",
    "\n",
    "        # 의도별 템플릿 선택\n",
    "        template = self.templates.get(analysis.intent, default_template)\n",
    "\n",
    "        # 체인 실행\n",
    "        chain = template | self.llm\n",
    "\n",
    "        response = chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"analysis\": analysis.model_dump_json(),\n",
    "            \"urgency\": analysis.urgency,\n",
    "            \"sentiment\": analysis.sentiment\n",
    "        })\n",
    "\n",
    "        return response.content"
   ],
   "id": "83879d79e5c9961",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 정리",
   "id": "e48183303e88f198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1) 입력에 대해 전처리 및 분석\n",
    "2) 병렬 분석 및 데이터 처리\n",
    "3) Routing 결정\n",
    "4) 최종 RAG Chain 확정\n",
    "\n",
    "### 문제점\n",
    "순차적인 진행(단방향), 1개라도 실패하면 되돌아올 수 없음(전체 실패)"
   ],
   "id": "6ae5a4ac29ced3f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
