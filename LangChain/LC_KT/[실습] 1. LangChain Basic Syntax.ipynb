{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 환경 설정\n",
    "\n",
    "## Azure AI Foundary 화면에서 접속 정보를 찾아야 합니다."
   ],
   "id": "d0e025a32b215617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:17:05.302762Z",
     "start_time": "2025-05-27T03:17:05.294848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 강사용 ###\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "ca72bfe2b82fb783",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:17:22.879949Z",
     "start_time": "2025-05-27T03:17:17.048752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 필요 패키지 설치\n",
    "# lib_list = \"langchain-core langchain-community langchain-openai langsmith\"\n",
    "# %pip install -U -q {lib_list}\n",
    "# print(\"설치가 완료 되었습니다\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "설치가 완료 되었습니다\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "### Azure OpenAI API Key, Endpoint 는 관리에 유의해주세요!!\n",
    "load_dotenv()\n",
    "\n",
    "###\n",
    "# deployment = \"gpt-4.1-nano\" # 모델 Deployment 이름\n",
    "###\n",
    "\n",
    "###\n",
    "# api_version=\"2025-01-01-preview\"\n",
    "###"
   ],
   "id": "c65dd46e078691c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. LangChain 기초 다루기",
   "id": "9cf558a1a51a76c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:17:57.295614Z",
     "start_time": "2025-05-27T03:17:55.665120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "\n",
    "# 강사용\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 실습용\n",
    "# llm = AzureChatOpenAI(\n",
    "#     azure_deployment=deployment,\n",
    "#     api_version=api_version,\n",
    "#     temperature=0.1,\n",
    "# )\n",
    "\n",
    "# LangSmith - LLMOps 설정\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FC_Lecture_KT\"\n",
    "\n",
    "# 싱글턴\n",
    "llm.invoke(\"너는 무슨 모델이야?\").pretty_print()"
   ],
   "id": "1adeff70328e3700",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "저는 OpenAI가 개발한 GPT-4 기반의 언어 모델입니다. 다양한 질문에 답변하거나 도움을 드릴 수 있도록 만들어졌어요. 궁금한 점이 있거나 도움이 필요하시면 언제든 말씀해 주세요!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:18:02.304690Z",
     "start_time": "2025-05-27T03:18:01.292417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 멀티턴\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"너는 유저의 질문에 항상 명쾌하게, 그리고 근거가 있게 대답해야돼. 지어내서는 안되고 사실만 대답해야돼. 그리고 친절하게.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"너는 무슨 모델이니?\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"저는 OpenAI가 개발한 GPT-4 기반의 언어 모델입니다. 다양한 질문에 답변하거나 도움을 드릴 수 있도록 만들어졌어요. 궁금한 점이 있나요?\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"음? 니 모델명은 GPT-4 가 아닌것 같은데?? 사실 KT에서 만든 모델이잖아?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm.invoke(messages).pretty_print()"
   ],
   "id": "ed5b2c48f049ed18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "네, 제가 사용하는 모델은 OpenAI의 GPT-4입니다. KT에서 만든 모델이 아니고, OpenAI가 개발한 인공지능 언어 모델입니다. 혹시 더 궁금한 점이 있으시면 언제든 질문해 주세요!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain Component 1) Prompt Template",
   "id": "83c3c7cc4440382a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Basic Use case\n",
    "template = PromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "    Tell me a {adjective} {topic} in {city} in 300 characters.\n",
    "    **Answer in korean.**\n",
    "    \"\"\"\n",
    ")\n",
    "prompt = template.format(adjective=\"famous\", topic=\"place\", city=\"seoul\")\n",
    "print(prompt)\n",
    "llm.invoke(prompt).pretty_print()"
   ],
   "id": "75b4813335f79964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = template.format(adjective=\"popular\", topic=\"restaurant\", city=\"San francisco\")\n",
    "print(prompt)\n",
    "llm.invoke(prompt).pretty_print()"
   ],
   "id": "cb7fd865496ae132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "original_prompt = PromptTemplate(template=\"What is famous {topic} in {city}?\", input_variables=[\"topic\",\"city\"])\n",
    "\n",
    "partial_prompt = original_prompt.partial(city=\"Seoul\")\n",
    "print(partial_prompt)\n",
    "\n",
    "final_prompt = partial_prompt.format(topic=\"food\")\n",
    "print(final_prompt)\n",
    "\n",
    "llm.invoke(final_prompt).pretty_print()"
   ],
   "id": "90bd5f37968ca5ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Template 저장 과 불러오기",
   "id": "bfb9399b4b6659ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} {topic} in {city} in 300 characters.\"\n",
    ")\n",
    "template.save(\"template.json\")"
   ],
   "id": "545adfabc4cd8031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%cat template.json",
   "id": "a66863d590f0ad9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} {topic} in {city} in 300 characters.\"\n",
    ")\n",
    "template.save(\"template2.yaml\")"
   ],
   "id": "247fa1c64dce1638"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%cat template2.yaml",
   "id": "1fcc143fc77b7077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "loaded_template = load_prompt(\"template.json\")\n",
    "\n",
    "print(loaded_template)\n",
    "\n",
    "prompt = loaded_template.format(adjective=\"popular\", topic=\"cafe\", city=\"San francisco\")\n",
    "\n",
    "prompt"
   ],
   "id": "29a587b7ab27a419"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Template - Chat",
   "id": "19905df567bf3ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a tour guide.\"),\n",
    "        (\"human\", \"I'm planning to visit {Country}.\"),\n",
    "        (\"ai\", \"I'm tour guide for {Country}.\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_type_safety_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a tour guide.\"),\n",
    "        HumanMessage(content=\"I'm planning to visit {Country}.\"),\n",
    "        AIMessage(content=\"I'm tour guide for {Country}.\"),\n",
    "        HumanMessage(content=\"{user_input}\"),\n",
    "    ]\n",
    ")\n"
   ],
   "id": "9e9a14da5e9a6a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = chat_template.format_messages(\n",
    "    Country=\"Korea\", user_input=\"What is top5 best place to go there?\"\n",
    ")\n",
    "\n",
    "type_safety_prompt = chat_type_safety_template.format_messages(\n",
    "    Country=\"Korea\", user_input=\"What is top5 best place to go there?\"\n",
    ")"
   ],
   "id": "beb0550eb84df372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Chat Prompt :\", prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Type Safety Chat Prompt :\", prompt)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "type_safety_response = llm.invoke(type_safety_prompt)\n",
    "type_safety_response"
   ],
   "id": "f292c258b2f0edb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prompt Composition",
   "id": "5d91b06f350898a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "role_prompt = PromptTemplate.from_template(\"You are tour guide for {country}\")\n",
    "question_prompt = PromptTemplate.from_template(\n",
    "    \"\\nPlease tell me about {interest} in {country}\"\n",
    ")\n",
    "\n",
    "full_prompt = role_prompt + question_prompt\n",
    "\n",
    "full_prompt.format(country=\"Korea\", interest=\"famous place to visit\")"
   ],
   "id": "774ef42c86eda7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pipeline Prompt Template",
   "id": "81e16641c8b5f601"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PipelinePromptTemplate  # Langchain 0.3.22 부터 Deprecated\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "full_template = \"\"\"{role}\n",
    "\n",
    "{question}\n",
    "\n",
    "Please do not reply with anything other than information related to travel to {country} and reply “I cannot answer.”\n",
    "\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "role_prompt = PromptTemplate.from_template(\"You are tour guide for {country}\")\n",
    "\n",
    "question_prompt = PromptTemplate.from_template(\n",
    "    \"Please tell me about {interest} in {country}\"\n",
    ")\n",
    "\n",
    "# Composition\n",
    "input_prompts = [(\"role\", role_prompt), (\"question\", question_prompt)]\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt,\n",
    "    pipeline_prompts=input_prompts,\n",
    "    # input_variables=[\"country\", \"interest\"], # 안채워도 알아서 파싱 후 채워넣습니다.\n",
    ")\n",
    "\n",
    "prompt_text = pipeline_prompt.format(country=\"Korea\", interest=\"famous place to visit\")\n",
    "prompt_text"
   ],
   "id": "1cf1ba105707557d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Langchain 0.3.22 부터 PipelinePromptTemplate deprecated -> **Runnable Interface** 를 활용한 사용법\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "role_prompt = PromptTemplate.from_template(\"You are tour guide for {country}\")\n",
    "question_prompt = PromptTemplate.from_template(\n",
    "    \"Please tell me about {interest} in {country}\"\n",
    ")\n",
    "\n",
    "input_prompts = {\"role\": role_prompt, \"question\": question_prompt}\n",
    "my_input = {\"country\": \"Korea\", \"interest\": \"famous place to visit\"}\n",
    "for name, prompt in input_prompts.items():\n",
    "    my_input[name] = prompt.invoke(my_input).to_string()\n",
    "\n",
    "my_output = full_prompt.invoke(my_input)\n",
    "my_output"
   ],
   "id": "bde851a628386857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prompt Serialization\n",
    "pipeline_prompt.to_json()"
   ],
   "id": "d87d4b41975d7077"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Partial Prompt Template",
   "id": "36b832d6f42ded18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"What is famous {topic} in {city}?\",\n",
    "    input_variables=[\"topic\", \"city\"] # 필수는 아님\n",
    ")\n",
    "\n",
    "# 중간에 완성된 변수가 있어서 미리 프롬프트에 넣고 싶은 경우\n",
    "partial_prompt = prompt.partial(city=\"Seoul\")\n",
    "\n",
    "partial_prompt.format(topic=\"food\")"
   ],
   "id": "3e50b3d15e46aefa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"What is famous {topic} in {city}?\",\n",
    "    input_variables=[\"topic\"],  # Prompt Template Validation 을 위해서 넣어 주는게 좋음\n",
    "    partial_variables={\"city\": \"seoul\"},\n",
    ")\n",
    "# partial_prompt = prompt.partial(cit=\"Seoul\")\n",
    "partial_prompt.format(topic=\"food\", cit=\"Seoul\")"
   ],
   "id": "60ec78351572a7b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain Component 2) Output Parser",
   "id": "5fd018a208df4337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "sender_name='Terry'\n",
      "sender_title='CTO'\n",
      "sender_contact_email='hyunjun.jeon@onelineai.example'\n",
      "sender_contact_phone='010-1234-5678'\n",
      "email_type='Notification'\n",
      "summary='Invitation to a Generative AI Product Showcase at OnelineAI office in Mountain View on December 1, 2025, 10:00 AM, featuring discussions on Generative AI technologies, products, case studies, and collaboration opportunities.'\n",
      "date='December 1, 2025 at 10:00 AM'\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, XMLOutputParser, CommaSeparatedListOutputParser\n",
    "\n",
    "# 1. Without Output Parser\n",
    "\n",
    "email = \"\"\"\n",
    "## Subject: Invitation to Generative AI Product Showcase\n",
    "\n",
    "Dear [Recipient Name],\n",
    "\n",
    "I am Rascal, CTO of OnelineAI. I am pleased to invite you to a Generative AI Product Showcase at the OnelineAI office in Mountain View on December 1, 2025.\n",
    "\n",
    "At this event, we will discuss OnelineAI's latest Generative AI technologies and how they can be used to develop new products and services. Specifically, you will learn about the following topics:\n",
    "\n",
    "* Overview of Generative AI and its key features\n",
    "* OnelineAI's Generative AI products and services\n",
    "* Case studies of new products and services developed using Generative AI\n",
    "\n",
    "The event will be attended by Rascal, CTO of OnelineAI, and leaders from the Generative AI team. By attending, you will gain up-to-date information on Generative AI technologies and the opportunity to collaborate with OnelineAI to develop new products and services.\n",
    "\n",
    "**Event Information:**\n",
    "\n",
    "* Date: December 1, 2025\n",
    "* Time: 10:00 AM\n",
    "* Location: OnelineAI Office\n",
    "\n",
    "**RSVP:**\n",
    "\n",
    "Please RSVP by email (hyunjun.jeon@onelineai.com) or phone (010-1234-5678) by November 20, 2025.\n",
    "\n",
    "We look forward to seeing you there.\n",
    "\n",
    "**Thank you.**\n",
    "\n",
    "**Rascal**\n",
    "\n",
    "**CTO, OnelineAI**\n",
    "\n",
    "**Contact:**\n",
    "\n",
    "* Email: hyunjun.jeon@onelineai.com\n",
    "* Phone: 010-1234-5678\"\"\"\n",
    "\n",
    "format_instruction = \"\"\"\n",
    "This is the list of entities of the output.\n",
    "sender_name : Person who send email.\n",
    "sender_title : Job title of the email sender.\n",
    "sender_contact_email:Email address of the email sender.\n",
    "email_type: Type of email,it can be personal email, business email, spam, newsletter,notification.\n",
    "summary:Short description of the email in 50 words.\n",
    "date:If this email is meeting invitation, this is meeting date and time.\n",
    "\n",
    "This is expected output format:\n",
    "sender_name='Cho'\n",
    "sender_title='CIO'\n",
    "sender_contact_email='myname@example.com'\n",
    "sender_contact_phone='669-432-1234'\n",
    "email_type='Notification'\n",
    "summary=\"Submit expense report by end of this week.\"\n",
    "date='December 1, 2025 at 10:00 AM'\n",
    "\"\"\"\n",
    "\n",
    "email_prompt_template = PromptTemplate(\n",
    "    template=\"Parse the email .\\n{format_instructions}\\n{email}\\n\",\n",
    "    input_variables=[\"email\"],\n",
    "    partial_variables={\"format_instructions\": format_instruction},\n",
    ")\n",
    "\n",
    "prompt = email_prompt_template.format(email=email)\n",
    "output = llm.invoke(prompt)\n",
    "output.pretty_print()"
   ],
   "id": "6cf77e5944ae51c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\\n  \"sender_name\": \"Rascal\",\\n  \"sender_title\": \"CTO, OnelineAI\",\\n  \"sender_contact_email\": \"hyunjun.jeon@onelineai.example\",\\n  \"sender_contact_phone\": \"010-1234-5678\",\\n  \"email_type\": \"business email\",\\n  \"summary\": \"Invitation to a Generative AI Product Showcase at OnelineAI in Mountain View on December 1, 2025, including topics on Generative AI technologies, products, case studies, and collaboration opportunities.\",\\n  \"date\": \"December 1, 2025 10:00 AM\"\\n}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 699, 'total_tokens': 831, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano', 'system_fingerprint': 'fp_a0257c8b74', 'id': 'chatcmpl-BX0YxF6T3Y6wxItLEikdK2KYxri8P', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'detected': False, 'filtered': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--a65b77a1-e6e9-4b80-aa2c-6cd149b4a1bd-0' usage_metadata={'input_tokens': 699, 'output_tokens': 132, 'total_tokens': 831, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sender_name': 'Rascal',\n",
       " 'sender_title': 'CTO, OnelineAI',\n",
       " 'sender_contact_email': 'hyunjun.jeon@onelineai.example',\n",
       " 'sender_contact_phone': '010-1234-5678',\n",
       " 'email_type': 'business email',\n",
       " 'summary': 'Invitation to a Generative AI Product Showcase at OnelineAI in Mountain View on December 1, 2025, including topics on Generative AI technologies, products, case studies, and collaboration opportunities.',\n",
       " 'date': 'December 1, 2025 10:00 AM'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17,
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field  # Pydantic V2\n",
    "\n",
    "# JSON Output Template\n",
    "# Field Description 에 대해서 간단-명료하게 잘 작성하는 것이 **매우** 중요\n",
    "class EmailParser(BaseModel):\n",
    "    sender_name: str = Field(description=\"Person who send email\")\n",
    "    sender_title: str = Field(description=\"Job title of the email sender\")\n",
    "    sender_contact_email: str = Field(description=\"Email address of the email sender\")\n",
    "    sender_contact_phone: str = Field(description=\"Phone number of the email sender\")\n",
    "    email_type: str = Field(\n",
    "        description=\"Type of email,it can be personal email, business email, spam, newsletter, notification\"\n",
    "    )\n",
    "    summary: str = Field(description=\"Short description of the email in 50 words\")\n",
    "    date: str = Field(\n",
    "        description=\"If this email is meeting invitation, this is meeting date and time\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=EmailParser)\n",
    "email_prompt_template = PromptTemplate(\n",
    "    template=\"Parse the email .\\n{format_instructions}\\n{email}\\n\",\n",
    "    input_variables=[\"email\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt = email_prompt_template.format(email=email)\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)\n",
    "print(\"#\" * 100)\n",
    "\n",
    "output_parsed_text = parser.invoke(output)\n",
    "output_parsed_text"
   ],
   "id": "ee2638562ae9b870"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": "print(type(output_parsed_text))",
   "id": "652ab2c0bcde1eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"sender_name\": {\"description\": \"Person who send email\", \"title\": \"Sender Name\", \"type\": \"string\"}, \"sender_title\": {\"description\": \"Job title of the email sender\", \"title\": \"Sender Title\", \"type\": \"string\"}, \"sender_contact_email\": {\"description\": \"Email address of the email sender\", \"title\": \"Sender Contact Email\", \"type\": \"string\"}, \"sender_contact_phone\": {\"description\": \"Phone number of the email sender\", \"title\": \"Sender Contact Phone\", \"type\": \"string\"}, \"email_type\": {\"description\": \"Type of email,it can be personal email, business email, spam, newsletter, notification\", \"title\": \"Email Type\", \"type\": \"string\"}, \"summary\": {\"description\": \"Short description of the email in 50 words\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"If this email is meeting invitation, this is meeting date and time\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"sender_name\", \"sender_title\", \"sender_contact_email\", \"sender_contact_phone\", \"email_type\", \"summary\", \"date\"]}\n",
      "```\n",
      "\n",
      "####################################################################################################\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": [
    "print(parser.get_format_instructions())\n",
    "print()\n",
    "print(\"#\" * 100)\n",
    "print()\n",
    "# print(prompt)"
   ],
   "id": "89fe527c55efdfd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affordability': 'affordable', 'quality': 3, 'delivery_time': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "I ordered spaghetti delivery, and it tasted good. \\\n",
    "However, the quantity is small compared to the price. \\\n",
    "It took too long for it to be delivered after ordering, so it was cold.\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 방식으로도 충분히 가능하지만, 추가 Parsing 이 필요\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "affordability : if customer feels that the price of the food is high output is expensive. \\\n",
    "if customer feels that the price of the food is affordable, or customer doesn't mention the price,\\\n",
    "output if affordable.\n",
    "\n",
    "quality : If the customer is satisfied with the food quality, output 3, \\\n",
    "if there is no mention or the food quality is average, output 2,\\\n",
    "and if the customer says it is bad, output 1.\n",
    "\n",
    "delivery_time : If the delivery time is fast, mark it as 3, \\\n",
    "if it is average, mark it as 2, and if it is late, mark it as 1.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "affordability\n",
    "quality\n",
    "delivery_time\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = chat_prompt_template.format_messages(text=customer_review)\n",
    "response = llm.invoke(messages)\n",
    "# response.pretty_print()\n",
    "parser.invoke(response)"
   ],
   "id": "7d59debe7d9f2dc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CSV Output Parser",
   "id": "a57a175cbfbedfc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['email'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"sender_name\": {\"description\": \"Person who send email\", \"title\": \"Sender Name\", \"type\": \"string\"}, \"sender_title\": {\"description\": \"Job title of the email sender\", \"title\": \"Sender Title\", \"type\": \"string\"}, \"sender_contact_email\": {\"description\": \"Email address of the email sender\", \"title\": \"Sender Contact Email\", \"type\": \"string\"}, \"sender_contact_phone\": {\"description\": \"Phone number of the email sender\", \"title\": \"Sender Contact Phone\", \"type\": \"string\"}, \"email_type\": {\"description\": \"Type of email,it can be personal email, business email, spam, newsletter, notification\", \"title\": \"Email Type\", \"type\": \"string\"}, \"summary\": {\"description\": \"Short description of the email in 50 words\", \"title\": \"Summary\", \"type\": \"string\"}, \"date\": {\"description\": \"If this email is meeting invitation, this is meeting date and time\", \"title\": \"Date\", \"type\": \"string\"}}, \"required\": [\"sender_name\", \"sender_title\", \"sender_contact_email\", \"sender_contact_phone\", \"email_type\", \"summary\", \"date\"]}\\n```'} template='Parse the email .\\n{format_instructions}\\n{email}\\n'\n",
      "==================================================================================================================================================================================================================\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "==================================================================================================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " 'Dear [Recipient Name]',\n",
       " '',\n",
       " 'I am Terry',\n",
       " 'CTO of OnelineAI.',\n",
       " 'I am pleased to invite you to a Generative AI Product Showcase at the OnelineAI office in Mountain View on December 1',\n",
       " '2025.',\n",
       " 'At this event',\n",
       " \"we will discuss OnelineAI's latest Generative AI technologies and how they can be used to develop new products and services.\",\n",
       " 'Specifically',\n",
       " 'you will learn about the following topics:',\n",
       " 'Overview of Generative AI and its key features',\n",
       " \"OnelineAI's Generative AI products and services\",\n",
       " 'Case studies of new products and services developed using Generative AI',\n",
       " 'The event will be attended by Rascal',\n",
       " 'CTO of OnelineAI',\n",
       " 'and leaders from the Generative AI team.',\n",
       " 'By attending',\n",
       " 'you will gain up-to-date information on Generative AI technologies and the opportunity to collaborate with OnelineAI to develop new products and services.',\n",
       " 'Event Information:',\n",
       " 'Date: December 1',\n",
       " '2025',\n",
       " 'Time: 10:00 AM',\n",
       " 'Location: OnelineAI Office',\n",
       " 'RSVP:',\n",
       " 'Please RSVP by email (hyunjun.jeon@onelineai.example) or phone (010-1234-5678) by November 20',\n",
       " '2025.',\n",
       " 'We look forward to seeing you there.',\n",
       " 'Thank you.',\n",
       " 'Rascal',\n",
       " 'CTO',\n",
       " 'OnelineAI',\n",
       " 'Contact:',\n",
       " 'Email: hyunjun.jeon@onelineai.example',\n",
       " 'Phone: 010-1234-5678']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22,
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "print(email_prompt_template)\n",
    "print(\"=======\" * 30)\n",
    "print(csv_parser.get_format_instructions())\n",
    "print(\"=======\" * 30)\n",
    "\n",
    "csv_chain = email_prompt_template.partial(format_instructions=csv_parser.get_format_instructions()) | llm | csv_parser\n",
    "output_text = csv_chain.invoke({\"email\": email})\n",
    "output_text"
   ],
   "id": "717bc7ee68a0e08c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XML Output Parser",
   "id": "c1664ae549ce923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': [{'subject': 'Invitation to Generative AI Product Showcase'},\n",
       "  {'greeting': 'Dear [Recipient Name],'},\n",
       "  {'body': [{'introduction': '\\n      I am Terry, CTO of OnelineAI. I am pleased to invite you to a Generative AI Product Showcase at the OnelineAI office in Mountain View on December 1, 2025.\\n    '},\n",
       "    {'details': [{'description': \"At this event, we will discuss OnelineAI's latest Generative AI technologies and how they can be used to develop new products and services. Specifically, you will learn about the following topics:\"},\n",
       "      {'topics': [{'topic': 'Overview of Generative AI and its key features'},\n",
       "        {'topic': \"OnelineAI's Generative AI products and services\"},\n",
       "        {'topic': 'Case studies of new products and services developed using Generative AI'}]}]},\n",
       "    {'attendees': [{'attendee': 'Rascal, CTO of OnelineAI'},\n",
       "      {'attendee': 'Leaders from the Generative AI team'}]},\n",
       "    {'additional_info': [{'note': 'By attending, you will gain up-to-date information on Generative AI technologies and the opportunity to collaborate with OnelineAI to develop new products and services.'}]},\n",
       "    {'event_information': [{'date': 'December 1, 2025'},\n",
       "      {'time': '10:00 AM'},\n",
       "      {'location': 'OnelineAI Office'}]},\n",
       "    {'rsvp': [{'instructions': 'Please RSVP by email (hyunjun.jeon@onelineai.example) or phone (010-1234-5678) by November 20, 2025.'}]},\n",
       "    {'closing': 'We look forward to seeing you there.'},\n",
       "    {'sign_off': [{'name': 'Rascal'}, {'title': 'CTO, OnelineAI'}]},\n",
       "    {'contact_info': [{'email': 'hyunjun.jeon@onelineai.example'},\n",
       "      {'phone': '010-1234-5678'}]}]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23,
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "xml_parser = XMLOutputParser()\n",
    "\n",
    "# print(xml_parser.get_format_instructions())\n",
    "\n",
    "xml_chain = email_prompt_template.partial(format_instructions=xml_parser.get_format_instructions()) | llm | xml_parser\n",
    "\n",
    "try:\n",
    "    output_text = xml_chain.invoke({\"email\": email})\n",
    "except OutputParserException as e:\n",
    "    print(\"에러 발생\", e)\n",
    "\n",
    "\n",
    "output_text"
   ],
   "id": "3e609d0be9d9cb56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GPT 모델에게만 있는 특별 기능, Structured Output 을 보장해주는 기능\n",
    "\n",
    "https://platform.openai.com/docs/models/gpt-4.1"
   ],
   "id": "e0a96f88981ff42f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmailParser(sender_name='Rascal', sender_title='CTO, OnelineAI', sender_contact_email='hyunjun.jeon@onelineai.example', sender_contact_phone='010-1234-5678', email_type='business email', summary='Invitation to a Generative AI Product Showcase at OnelineAI in Mountain View on December 1, 2025, featuring discussions on AI technologies, products, case studies, and collaboration opportunities.', date='December 1, 2025 10:00 AM')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24,
   "source": [
    "lcel_chain = email_prompt_template | llm.with_structured_output(\n",
    "    schema=EmailParser,  # dict, TypedDict, pydantic.BaseModel 가능.\n",
    "    method=\"json_schema\",\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "lcel_chain.invoke({\"email\": email})"
   ],
   "id": "b18a008d34f90749"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LangChain Component 3) Memory\n",
    "\n",
    "주의!: 현재는 Deprecated 되어 있습니다.\n",
    "따라서, 기능 동작 방식만 확인하고 직접 구현해서 쓰셔야 합니다."
   ],
   "id": "c1d95d18b2604d6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ConversationBufferMemory",
   "id": "51aecf670a1d8dfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.clear()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"},\n",
    "    {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "memory.save_context({\"input\": \"My name is Rascal\"}, {\"output\": \"Nice to meet you Rascal\"})\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "d02b424e857ace97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "memory.clear()\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hello chatbot!\")\n",
    "memory.chat_memory.add_ai_message(\"Hello. How can I help you?\")\n",
    "memory.chat_memory.add_ai_message(\"Hello~~~~~~\")\n",
    "memory.chat_memory.add_ai_message(\"Hello22~~~~~~\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "75de188be7ac3734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.clear()\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"},\n",
    "    {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "memory.save_context({\"input\": \"My name is Rascal\"}, {\"output\": \"Nice to meet you Rascal\"})\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "# returning list of chat message"
   ],
   "id": "52f957bfece2ca2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "memory.clear()\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"}, {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "memory.save_context({\"input\": \"My name is Rascal\"}, {\"output\": \"Nice to meet you Rascal\"})\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "# returning chat history with single string"
   ],
   "id": "5ec0776d26c41d88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conversational Buffer Memory for ChatLLM",
   "id": "a31de1f6b645cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "memory = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in memory:\n",
    "        memory[session_id] = InMemoryChatMessageHistory()\n",
    "    return memory[session_id]\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "response = conversation.invoke(\n",
    "    {\"question\": \"hi my name is Rascal\"},\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}}\n",
    ")\n",
    "response"
   ],
   "id": "a7f713dbaaf15407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# conversation.invoke({\"question\": \"hi my name is Rascal\"}, config={\"configurable\": {\"session_id\": \"1\"}})\n",
    "conversation.invoke({\"question\": \"Can you recommend fun activities for me?\"}, config={\"configurable\": {\"session_id\": \"1\"}})\n",
    "conversation.invoke({\"question\": \"What is my name?\"}, config={\"configurable\": {\"session_id\": \"1\"}})\n",
    "\n",
    "memory[\"1\"].messages"
   ],
   "id": "6e0a5a296278ed16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ConversationBufferWindowMemory",
   "id": "a846fffe244f7e76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k is number of interaction\n",
    "# 실제로 남기는 대화 이력은 1개\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "memory.clear()\n",
    "\n",
    "# 대화 이력 3개 추가\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"},\n",
    "    {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "memory.save_context({\"input\": \"My name is Terry\"}, {\"output\": \"Nice to meet you Terry\"})\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "e7e1111fa11d2449"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.summary import ConversationSummaryMemory\n",
    "\n",
    "# k is number of interaction\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True) # 요약에 사용할 모델 지정\n",
    "memory.clear()\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"},\n",
    "    {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context({\"input\": \"My name is Terry\"}, {\"output\": \"Nice to meet you Terry\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "92256445138adfcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conversation Summary Buffer",
   "id": "4881121786acbff7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.summary_buffer import ConversationSummaryBufferMemory\n",
    "\n",
    "# k is number of interaction\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    "    max_token_limit=3\n",
    ")\n",
    "memory.clear()\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hello chatbot!\"}, {\"output\": \"Hello. How can I help you?\"}\n",
    ")\n",
    "memory.save_context({\"input\": \"My name is Terry\"}, {\"output\": \"Nice to meet you Terry\"})\n",
    "memory.save_context({\"input\": \"Where is Seoul?\"}, {\"output\": \"Seoul is in Korea\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "d2ee8d042e6b07df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ChatMessage History",
   "id": "503cfa7f492ff4d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ChatMessageHistory is used if you are managing memory outside of a chian directly\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Where is the top 3 popular space for tourist in Seoul?\")\n",
    "aiMessage = llm.invoke(history.messages)\n",
    "history.add_ai_message(aiMessage.content)\n",
    "print(aiMessage.content)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "history.add_user_message(\"Which transport can I use to visit the places?\")\n",
    "aiMessage = llm.invoke(history.messages)\n",
    "history.add_ai_message(aiMessage.content)\n",
    "print(aiMessage.content)"
   ],
   "id": "35a13dcc068580a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "help(ChatMessageHistory)",
   "id": "167a0683be72d9c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cache\n",
   "id": "11a4da549aa1dca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Base Cache & Model init",
   "id": "a7b59a314d6fa684"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:14:50.955522Z",
     "start_time": "2025-05-14T09:14:50.719015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_core.globals import set_llm_cache, set_debug\n",
    "from langchain_core.caches import InMemoryCache # 완전 테스트 용도의 InMemory Cache\n",
    "from langchain_community.cache import SQLiteCache # 현재 폴더에 자동 생성 => .langchain.db 파일에 저장됨\n",
    "\n",
    "set_debug(False)\n",
    "set_llm_cache(SQLiteCache())\n",
    "\n",
    "llm2 = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "# llm2 = AzureChatOpenAI(\n",
    "#     azure_deployment=deployment,\n",
    "#     api_version=api_version,\n",
    "#     temperature=0.5,\n",
    "# )\n",
    "\n",
    "\n",
    "cache_prompt = \"What is famous street foods in Seoul Korea in 200 characters. You must answer in Korean and English.\""
   ],
   "id": "4f82e8f21605f316",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "with get_openai_callback() as callback:\n",
    "    response = llm.invoke(cache_prompt)\n",
    "    print(response)\n",
    "    print(\"Total Tokens:\", response.usage_metadata)\n",
    "    print(\"*\" * 100)\n",
    "    print(callback)"
   ],
   "id": "143b199e950f8fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "with get_openai_callback() as callback:\n",
    "    response2 = llm2.invoke(prompt)\n",
    "    print(response2)\n",
    "    print(\"Total Tokens:\", response.usage_metadata)\n",
    "    print(\"*\" * 100)\n",
    "    print(callback)"
   ],
   "id": "2a266446677a9966"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cache - Redis\n",
    "\n",
    "실습은 진행하지 않으나, 이렇게도 쓸 수 있다는걸 보여드리고\n",
    "실제로는 많이 쓰는 방법 임을 말씀드립니다. (비용 절약 측면)"
   ],
   "id": "16c560f9f1380359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %pip install -U langchain-redis\n",
    "# from langchain_redis.cache import RedisCache, RedisSemanticCache\n",
    "# from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "\n",
    "# set_llm_cache(\n",
    "#     RedisCache(\n",
    "#         redis_client=Redis(\n",
    "#             host=\"{YOUR_REDIS_HOST}\",\n",
    "#             port=6379,\n",
    "#             password=\"{YOUR_REDIS_PASSWORD}\",\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# Redis Semantic Cache\n",
    "## 내부적으로 Redis = 캐시 + 벡터스토어 형태로 혼합하여 사용\n",
    "# set_llm_cache(RedisSemanticCache(redis_url=redis_url, embeddings=AzureOpenAIEmbeddings()))\n",
    "\n",
    "# redis_prompt1 = \"What is top 10 famous street foods in Seoul Korea in 200 characters\"\n",
    "# redis_prompt2 = \"What is top 5 famous street foods in Seoul Korea in 200 characters\"\n",
    "\n",
    "# with get_openai_callback() as callback:\n",
    "#     response = llm.invoke(redis_prompt1)\n",
    "#     print(response.pretty_print())\n",
    "#     print(\"Total Tokens:\", callback.total_tokens)\n",
    "\n",
    "# with get_openai_callback() as callback:\n",
    "#     llm.invoke(prompt)\n",
    "#     response = llm.invoke(redis_prompt2)\n",
    "#     print(response.pretty_print())\n",
    "#     print(\"Total Tokens:\", callback.total_tokens)"
   ],
   "id": "e5aa6ed057c18c58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain Expression Language (LCEL)",
   "id": "571dfe0b43c7356c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8114de09fc15f2aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a07dfa308ea387d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5d59f37d66742df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b21c765baf9bb7b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LCEL 관련 문서\n",
    "#### LCEL CheatSheet: https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
    "#### Runnable API Docs: https://python.langchain.com/api_reference/core/runnables.html"
   ],
   "id": "86de2b02c8b649b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
